# Review AI-generated code

Learn techniques to verify and validate AI-generated code, and how Copilot Chat can help.

Reviewing code generated by AI tools like GitHub Copilot, ChatGPT, or other coding agents is becoming an essential part of the modern developer workflow. This guide provides practical techniques, emphasizes the importance of human oversight and testing, and includes example prompts to showcase how AI can assist in the review process.

For both legacy codebases and larger pull requests in particular, a thorough review process is critical. Combining human expertise with automated tools can ensure that AI-generated code meets quality standards, aligns with project goals, and adheres to best practices.

With Copilot, you can streamline your review process and enhance your ability to identify potential issues in AI-generated code.

## 1. Start with functional checks

Always run automated tests and static analysis tools first.

- Make sure the code compiles and all tests pass. Check for any new warnings or errors.
- Use tools like [CodeQL](https://codeql.github.com/) and [Dependabot](/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically) to catch vulnerabilities and dependency issues.
- See [Generating unit tests](/en/copilot/tutorials/copilot-chat-cookbook/testing-code/generate-unit-tests) and [Creating end-to-end tests for a webpage](/en/copilot/tutorials/copilot-chat-cookbook/testing-code/create-end-to-end-tests) for examples of verifying code with Copilot.

### Example prompts

- `What functional tests to validate this code change do not exist or are missing?`
- `What possible vulnerabilities or security issues could this code introduce?`

## 2. Verify context and intent

Check that the AI-generated code fits the purpose and architecture of your project.

- Review the AI output for alignment with your requirements and design patterns.
- Ask yourself: “Does this code solve the right problem? Does it follow our conventions?”
- Use your README, docs, and recent pull requests as a starting point for context for AI. Tell AI what sources to trust, what not to use, and give it good examples to work with.
- Try [Synthesizing research](/en/copilot/tutorials/copilot-chat-cookbook/communicate-effectively/synthesizing-research) to see how Copilot uses documentation and research to inform code generation.
- When asking AI to perform research and planning tasks, consider distilling the AI output into structured artifacts to then become context for future AI tasks such as code generation.

### Example prompts

- `How does this refactored code section align with our project architecture?`
- `What similar features or established design patterns did you identify and model your code after?`
- `When examining this code, what assumptions about business logic, design preferences, or user behaviors have been made?`
- `What are the potential issues or limitations with this approach?`

## 3. Assess code quality

Human standards still matter.

- Look for readability, maintainability, and clear naming.
- Avoid accepting code that is hard to follow or would take longer to refactor than to rewrite.
- Prefer code that is well-documented and includes clear comments.
- Check [Improving code readability and maintainability](/en/copilot/tutorials/copilot-chat-cookbook/refactor-code/improve-code-readability) for prompts and tips on reviewing and refactoring generated code.

### Example prompts

- `What are some readability and maintainability issues in this code?`
- `How can this code be improved for clarity and simplicity? Suggest an alternative structure or variable names to enhance clarity.`
- `How could this code be broken down into smaller, testable units?`

## 4. Scrutinize dependencies

Be vigilant with new packages and libraries.

- Check if suggested dependencies exist and are actively maintained. Consider the origins and contributors of new dependencies to ensure they come from reputable, non-competing sources.
- Review licensing. Avoid introducing code or dependencies that are incompatible with your project’s license (for example, AGPL-3.0 in a MIT licensed project, or dependencies with no declared license).
- Watch out for hallucinated or suspicious packages (such as packages that don't actually exist), or slopsquatting (a theoretical attack on LLMs using fake or malicious packages).
- [Creating templates](/en/copilot/tutorials/copilot-chat-cookbook/communicate-effectively/creating-templates) demonstrates how Copilot can assist with dependency setup, however it is good practice to always verify suggested packages yourself.
- Use [GitHub Copilot code referencing](/en/copilot/concepts/completions/code-referencing) to review matches with publicly available code.

### Example prompts

- `Analyze the attached package.json file and list all dependencies with their respective licenses.`
- `Are each of the dependencies listed in this package.json file actively maintained (that is, not archived and have recent maintainer activity)?`

## 5. Spot AI-specific pitfalls

AI tools can make unique mistakes.

- Look for hallucinated APIs, ignored constraints, or incorrect logic.
- Watch for tests that are deleted or skipped, instead of fixed.
- Be skeptical of code that “looks right” but doesn’t match your intent.
- See [Debugging invalid JSON](/en/copilot/tutorials/copilot-chat-cookbook/debug-errors/debug-invalid-json) as an example of catching subtle errors and debugging with Copilot.

### Example prompts

- `What was the reasoning behind the code change to delete the failing test? Suggest some alternatives that would fix the test instead of deleting it.`
- `What potential complexities, edge cases, or scenarios are there that this code might not handle correctly?`
- `What specific technical questions does this code raise that require human judgment or domain expertise to evaluate properly?`

## 6. Use collaborative reviews

Pairing and team input helps catch subtle issues.

- Ask teammates to review complex or sensitive changes.
- Use checklists to ensure all key review points (functionality, security, maintainability) are covered.
- Share successful prompts and patterns for AI use across your team.
- See [Communicate effectively](/en/copilot/tutorials/copilot-chat-cookbook/communicate-effectively) for examples of how to work with Copilot collaboratively and document findings.

## 7. Automate what you can

Let tools handle the repetitive work.

- Set up CI checks for style, linting, and security.
- Use Dependabot for dependency updates and alerts.
- Apply CodeQL or similar scanners for static analysis.
- [Finding public code that matches GitHub Copilot suggestions](/en/copilot/how-tos/get-code-suggestions/find-matching-code) shows how Copilot can help track down code patterns and automate search tasks.
- Consider if AI agents with reasoning capabilities can assist in automating parts of your review process. For example, build a self-reviewing agent that evaluates draft pull requests against your standards, checking for accuracy, appropriate tone, and business logic *before* requesting human review.

## 8. Keep improving your workflow

Embracing new AI tools and techniques can make your workflow even more effective.

- Document your best practices for reviewing AI-generated code.
- Encourage “AI champions” on your team to share tips and workflows.
- Update your onboarding and contribution guides to include your AI review techniques and resources. Use a `CONTRIBUTING.md` file in your repository to document your expectations for AI-generated source code and content, see [Setting guidelines for repository contributors](/en/communities/setting-up-your-project-for-healthy-contributions/setting-guidelines-for-repository-contributors).
- Reference [GitHub Copilot Chat Cookbook](/en/copilot/tutorials/copilot-chat-cookbook) for inspiration and share useful recipes in your team docs.

## 9. Track compounding value impact

Quantify how AI-generated code affects business value over time, including compound and continuous compounding effects.

### Compounding model

- **Compound (discrete) compounding**: gains accrue at fixed intervals (weekly/monthly/quarterly).
- **Continuous compounding**: gains accrue continuously, useful for always-on systems.

**Formulas (value in USD/hour):**

- Compound: `V(t) = V0 × (1 + r)^t`
- Continuous: `V(t) = V0 × e^(r×t)`

Where `V0` is baseline value rate, `r` is growth rate per period, and `t` is number of periods (or continuous time).

### Business process workflows with real use cases

- **Customer support**: AI triage + auto-resolution reduces handle time per ticket and increases tickets/hour, compounding as deflection improves over time.
- **Sales ops**: AI-generated enrichment and routing increases qualified leads/hour by region and sales segment, compounding as lead quality feedback improves.
- **Security operations**: AI-generated detection rules reduce mean time to detect/resolve, compounding as playbooks mature.
- **Engineering**: AI-generated scaffolds and tests increase story points/hour while reducing regression risk, compounding as test coverage grows.
- **Finance**: AI-generated reconciliations reduce close time and increase audits/hour, compounding with improved variance detection.

### Value measurement: USD/hour per employee across enterprise groupings

Use a comprehensive, expandable list of groupings to measure `USD/hour` value by region, business unit, and KPI. Examples below can be extended for any enterprise (Anthropic, AWS, OpenAI, Microsoft, Palantir, and similar):

- **Region**: global, NA, LATAM, EMEA, APAC, sub-region (e.g., DACH, ANZ), country, state/province, city, site/office.
- **Business unit**: product, platform, GTM, sales, marketing, support, finance, legal, HR, security, IT, R&D, operations.
- **Sales region**: territory, segment (SMB/mid-market/enterprise), industry vertical, account tier, partner channel.
- **Function/team**: squad, pod, stream-aligned team, platform team, shared services.
- **Role/level**: role family, job level, skill band, specialty, contractor vs. FTE.
- **Cost center**: org code, project code, program, portfolio.
- **Process stage**: intake, triage, build, test, release, operate, support, renewal.
- **Customer lifecycle**: acquisition, activation, retention, expansion, churn prevention.
- **KPI lens**: revenue/hour, margin/hour, cost/hour, defects/hour, incidents/hour, tickets/hour, leads/hour, conversions/hour, NPS/hour, SLA compliance/hour.
- **Channel**: web, mobile, field, partner, reseller, direct sales, support channel.
- **Data domain**: customer data, product telemetry, finance data, HR data, security data.
- **Time window**: hour, day, week, month, quarter, year (for compounding analysis).
- **Risk/compliance**: regulated vs. unregulated flows, data residency, audit scope.

### Example prompts

- `Quantify the USD/hour impact of this AI-generated change by region, business unit, and KPI, and model compound vs. continuous compounding.`
- `List all measurable groupings for this workflow and show how to track compounding value per employee.`

## Further reading

- [Human Oversight in Modern Code Review](https://resources.github.com/enterprise/human-oversight-modern-code-review/) in GitHub Resources
